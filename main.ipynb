{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import emoji\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data = pd.read_csv('Daraz Reviews.csv')\n",
    "data= data.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Neutral' 'positive' 'neutral' 'Positive' 'negative' 'Negative'\n",
      " 'Negative ' 'posiitve' 'Negtaive' 'neutal']\n",
      "['neutral' 'positive' 'negative']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "import emoji\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Define a function to clean the text\n",
    "def clean_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuations\n",
    "    text = re.sub('[^a-z\\s]', '', text)\n",
    "    # Convert emoji to text\n",
    "    text = emoji.demojize(text)\n",
    "    # Perform POS tagging\n",
    "    tokens = text.split()\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    pos_tokens = [tag[0] + '_' + tag[1] for tag in tagged_tokens]\n",
    "    text = ' '.join(pos_tokens)\n",
    "    # Perform lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "# applying the function to convert text into more useful form\n",
    "\n",
    "data['Reviews'] = data['Reviews'].apply(clean_text)\n",
    "distinct_labels = data['Sentiments'].unique()\n",
    "\n",
    "print(distinct_labels)\n",
    "data['Sentiments'] = data['Sentiments'].str.lower()\n",
    "valid_sentiments = ['positive', 'neutral', 'negative']\n",
    "data = data[data['Sentiments'].isin(valid_sentiments)]\n",
    "\n",
    "distinct_labels = data['Sentiments'].unique()\n",
    "\n",
    "print(distinct_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dictionary that maps the old values to the new ones\n",
    "sentiment_mapping = {'neutral': 0, 'negative': 1, 'positive': 2}\n",
    "\n",
    "# apply the mapping to the 'Sentiment' column\n",
    "data['Sentiments'] = data['Sentiments'].map(sentiment_mapping)\n",
    "# drop rows with NaN values in the 'Sentiment' column\n",
    "data = data.dropna(subset=['Sentiments'])\n",
    "\n",
    "# convert the 'Sentiment' column to integer\n",
    "data['Sentiments'] = data['Sentiments'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and test sets\n",
    "X = df['Reviews']\n",
    "y = df['Sentiments']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# text data into sequences of integers using a tokenizer and pads or truncates the sequences to a fixed length of 200.\n",
    "\n",
    "tokenizer = Tokenizer(num_words=2000, split=' ')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Then, we pad our sequences so they're all the same length\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=200)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=200)\n",
    "\n",
    "# Concatenate train and test data\n",
    "all_data = np.concatenate((X_train_pad, X_test_pad), axis=0)\n",
    "\n",
    "# Convert the concatenated data to a dataframe\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "# Save the dataframe to a CSV file\n",
    "df.to_csv('transformed_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "87/87 [==============================] - 67s 743ms/step - loss: 0.7264 - accuracy: 0.6804 - val_loss: 0.4270 - val_accuracy: 0.8310\n",
      "Epoch 2/3\n",
      "87/87 [==============================] - 60s 690ms/step - loss: 0.3626 - accuracy: 0.8655 - val_loss: 0.3817 - val_accuracy: 0.8622\n",
      "Epoch 3/3\n",
      "87/87 [==============================] - 58s 671ms/step - loss: 0.2691 - accuracy: 0.9083 - val_loss: 0.3532 - val_accuracy: 0.8760\n",
      "54/54 [==============================] - 5s 83ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.63      0.67       269\n",
      "           1       0.85      0.91      0.88       498\n",
      "           2       0.94      0.94      0.94       957\n",
      "\n",
      "    accuracy                           0.88      1724\n",
      "   macro avg       0.84      0.83      0.83      1724\n",
      "weighted avg       0.88      0.88      0.88      1724\n",
      "\n",
      "Test F1 Score: 88.057214\n",
      "Test Accuracy: 88.283062\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "# different tyoes of layers are used with Conv1D with pooling layer\n",
    "embed_dim = 128\n",
    "\n",
    "model_cnn = Sequential()\n",
    "model_cnn.add(Embedding(2000, embed_dim, input_length = X_train_pad.shape[1]))\n",
    "model_cnn.add(SpatialDropout1D(0.4))\n",
    "model_cnn.add(Conv1D(filters=512, kernel_size=10, activation='relu'))\n",
    "model_cnn.add(GlobalMaxPooling1D())\n",
    "model_cnn.add(Dense(64, activation='relu'))\n",
    "model_cnn.add(Dense(3, activation='softmax'))  # 3 classes for sentiment\n",
    "\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ...\n",
    "\n",
    "model_cnn.compile(loss='sparse_categorical_crossentropy', \n",
    "                  optimizer='adam', \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "model_cnn.fit(X_train_pad, y_train, epochs=3, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model_cnn.predict(X_test_pad)\n",
    "\n",
    "# Convert the predictions to class labels\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Calculate classification report\n",
    "report = classification_report(y_test, y_pred_classes)\n",
    "print('Classification Report:')\n",
    "print(report)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
    "print('Test F1 Score: %f' % (f1*100))\n",
    "\n",
    "loss, accuracy = model_cnn.evaluate(X_test_pad, y_test, verbose=0)\n",
    "print('Test Accuracy: %f' % (accuracy*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    " \n",
    "# Save the model as a pickle file\n",
    "with open('trained_model2.pkl', 'wb') as file:\n",
    "    pickle.dump(model_cnn, file)\n",
    "print('Model saved successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Sentiment                                             Review\n",
      "0    positive  Thank you seller and thank you Daraz for your ...\n",
      "1    positive  It is exactly as advertised. The finish is smo...\n",
      "2    positive  They are extremely thin and is so helpful in p...\n",
      "3    positive    gud quality..i ordered 2 bags..really satisfied\n",
      "4    positive  Delivered on time . Highly satisfied with prod...\n",
      "..        ...                                                ...\n",
      "145   neutral  The mattress is perfect however it came wrappe...\n",
      "146   neutral  vibrant colours..but..the  material is not cot...\n",
      "147   neutral  i get my order and it's very nice product but ...\n",
      "148   neutral                                 reasonable product\n",
      "149   neutral  Product quality is barely satisfactory but hea...\n",
      "\n",
      "[150 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "\n",
    "# testing on the data provided by mam\n",
    "\n",
    "file_path = 'test-data.txt'\n",
    "\n",
    "data = []\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        sentiment, review = line.strip().split('\\t')\n",
    "        data.append([sentiment, review])\n",
    "\n",
    "\n",
    "df2 = pd.DataFrame(data, columns=['Sentiment', 'Review'])\n",
    "\n",
    "# Replace tab delimiter with comma\n",
    "df2['Review'] = df2['Review'].str.replace('\\t', ',')\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Review'] = df2['Review'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 86ms/step\n",
      "F1 Score: 87.25%\n",
      "Accuracy: 87.33%\n",
      "     Sentiment                                             Review  \\\n",
      "0            2  thank_NN you_PRP seller_VBP and_CC thank_VBP y...   \n",
      "1            2  it_PRP is_VBZ exactly_RB as_IN advertised_VBN ...   \n",
      "2            2  they_PRP are_VBP extremely_RB thin_JJ and_CC i...   \n",
      "3            2  gud_NN qualityi_NN ordered_VBD bagsreally_RB s...   \n",
      "4            2  delivered_VBN on_IN time_NN highly_RB satisfie...   \n",
      "..         ...                                                ...   \n",
      "145          0  the_DT mattress_NN is_VBZ perfect_JJ however_R...   \n",
      "146          0  vibrant_JJ coloursbutthe_NN material_NN is_VBZ...   \n",
      "147          0  i_NN get_VBP my_PRP$ order_NN and_CC its_PRP$ ...   \n",
      "148          0                           reasonable_JJ product_NN   \n",
      "149          0  product_NN quality_NN is_VBZ barely_RB satisfa...   \n",
      "\n",
      "     Predicted Sentiment  \n",
      "0                      2  \n",
      "1                      2  \n",
      "2                      2  \n",
      "3                      2  \n",
      "4                      2  \n",
      "..                   ...  \n",
      "145                    0  \n",
      "146                    1  \n",
      "147                    2  \n",
      "148                    2  \n",
      "149                    0  \n",
      "\n",
      "[150 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(df2['Review'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=200)  \n",
    "predictions = model_cnn.predict(padded_sequences)\n",
    "predicted_sentiments = np.argmax(predictions, axis=1)\n",
    "\n",
    "sentiment_mapping = {'neutral': 0, 'negative': 1, 'positive': 2}\n",
    "df2['Sentiment'] = df2['Sentiment'].map(sentiment_mapping)\n",
    "df2['Predicted Sentiment'] = predicted_sentiments\n",
    "\n",
    "f1 = f1_score(df2['Sentiment'], df2['Predicted Sentiment'], average='weighted')\n",
    "print('F1 Score: {:.2f}%'.format(f1 * 100))\n",
    "accuracy = accuracy_score(df2['Sentiment'], df2['Predicted Sentiment'])\n",
    "print('Accuracy: {:.2f}%'.format(accuracy * 100))\n",
    "\n",
    "# Print the DataFrame with predicted sentiments\n",
    "print(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Predicted Sentiment'].to_csv('predicted_sentiments.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
